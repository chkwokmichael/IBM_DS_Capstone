{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Data Science Professional Certificate\n",
    "\n",
    "## Capstone Project\n",
    "\n",
    "#### This is the Applied Data Science Capstone Project notebook created by <a href=\"https://www.linkedin.com/in/chin-hung-kwok-07927b152\">Chin Hung Kwok</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction/Business Problem\n",
    "\n",
    "Background: <br>\n",
    "Car Accidents are caused by a lot of different factors, such as weather conditions, time of the day, light conditions or road conditions. Once car accident happened, police and ambulance have to arrive as soon as possible to handle the case. Without any prediction, police and ambulanceman might not be able to be prepared for the case. Therefore, I would like to create a map which fetches the real-time conditions of the traffic and weather and shows the predicted severity of car accidents that might happen. Police and Ambulance might make use of this map to be prepared for the potential accidents.\n",
    "\n",
    "_Given day of the week, time, weather, light and road conditions, predict accident severity within the operating geographic area of a police force and ambulance._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "\n",
    "##### UK Accidents 10 years history with many variables\n",
    "<br>\n",
    "link: <a href='https://www.kaggle.com/benoit72/uk-accidents-10-years-history-with-many-variables'>https://www.kaggle.com/benoit72/uk-accidents-10-years-history-with-many-variables</a>\n",
    "\n",
    "The data source consists of three csv files, namely Accidents0514, Causualties0514, and Vehicles0514. It includes the data for the accidents, casualties, vehicles, respectively.\n",
    "\n",
    "In this project, the data file \"Accidents0514\" and \"Vehicles0514\" are mainly used.\n",
    "\n",
    "It includes 32 variables, such as the accident index, the longtidues and latitudes, speed limit, etc. Independents chosen for this project is namely:\n",
    "\n",
    "1. Day of week\n",
    "2. Time (hour)\n",
    "3. Light Condition\n",
    "4. Weather Conditions\n",
    "5. Road conditions\n",
    "6. Number of Vehicles\n",
    "7. Sex of Driver\n",
    "8. Age band of Driver\n",
    "9. 1st point of impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_acc = pd.read_csv('data/accidents.csv')\n",
    "df_veh = pd.read_csv('data/vehicles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_agg = pd.merge(df_acc,df_veh,how='inner',on='Accident_Index')\n",
    "df_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg['Accident_Severity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation/Exploration\n",
    "\n",
    "1. Day of week\n",
    "2. Time (hour)\n",
    "3. Light Condition\n",
    "4. Weather Conditions\n",
    "5. Road conditions\n",
    "6. Number of Vehicles\n",
    "7. Sex of Driver\n",
    "8. Age band of Driver\n",
    "9. 1st point of impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = df_agg['Accident_Severity'].value_counts().plot(kind='bar',\n",
    "                                                     color=['forestgreen', 'darkorange', 'mediumblue'],\n",
    "                                                     figsize=(10, 6))\n",
    "ax.set_xticklabels(['Slight','Serious','Fatal'])\n",
    "ax.set_title('Distribution of the Car Accident Severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_acc_dow = pd.pivot_table(df_agg, values='Accident_Index', \\\n",
    "                            index=['Accident_Severity'], \\\n",
    "                            columns=['Day_of_Week'], \\\n",
    "                            aggfunc=np.count_nonzero).transpose()\n",
    "ax = df_acc_dow.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg['Time'] = pd.to_datetime(df_agg['Time']).dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = pd.pivot_table(df_agg, values='Accident_Index', \\\n",
    "                            index=['Accident_Severity'], \\\n",
    "                            columns=['Time'], \\\n",
    "                            aggfunc=np.count_nonzero).transpose()\n",
    "\n",
    "df_acc_time.plot(kind='bar',stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_light = pd.pivot_table(df_agg, values='Accident_Index', \\\n",
    "                            index=['Accident_Severity'], \\\n",
    "                            columns=['Light_Conditions'], \\\n",
    "                            aggfunc=np.count_nonzero).transpose()\n",
    "ax = df_acc_light.plot(kind='bar',stacked=True)\n",
    "\n",
    "light_lookup = pd.read_excel(\"data/metadata.xls\",sheet_name=\"Light Conditions\")\n",
    "xticks = light_lookup['label'].values[0:len(light_lookup['code'].values)-1]\n",
    "ax.set_xticklabels(xticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.drop(df_agg[df_agg['Weather_Conditions'] == -1].index)['Weather_Conditions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_weather = pd.pivot_table(df_agg.drop(df_agg[df_agg['Weather_Conditions'] == -1].index), values='Accident_Index', \\\n",
    "                            index=['Accident_Severity'], \\\n",
    "                            columns=['Weather_Conditions'], \\\n",
    "                            aggfunc=np.count_nonzero).transpose()\n",
    "\n",
    "ax = df_acc_weather.plot(kind='bar',stacked=True)\n",
    "\n",
    "weather_lookup = pd.read_excel(\"data/metadata.xls\",sheet_name=\"Weather\")\n",
    "xticks = np.insert(weather_lookup['label'].values[0:len(weather_lookup['code'].values)-1],0,'')\n",
    "ax.set_xticklabels(xticks)\n",
    "ax.minorticks_off()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc_sl = pd.pivot_table(df_acc, values='Accident_Index', \\\n",
    "                            index=['Accident_Severity'], \\\n",
    "                            columns=['Speed_limit'], \\\n",
    "                            aggfunc=np.count_nonzero).transpose()\n",
    "\n",
    "df_acc_sl.plot(kind='bar',stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.pivot_table(df_agg.drop(df_agg[(df_agg['Sex_of_Driver'] == -1) | (df_agg['Sex_of_Driver'] == 3)].index), values='Accident_Index',\n",
    "                index=['Accident_Severity'],\n",
    "                columns=['Sex_of_Driver'],\n",
    "                aggfunc=np.count_nonzero).transpose().plot(kind='bar',stacked=True)\n",
    "\n",
    "sex_lookup = pd.read_excel(\"data/metadata.xls\",sheet_name=\"Sex of Driver\")\n",
    "xticks = sex_lookup['label'].values[0:len(sex_lookup['code'].values)-2]\n",
    "ax.set_xticklabels(xticks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.pivot_table(df_agg.drop(df_agg[df_agg['1st_Point_of_Impact'] == -1].index), values='Accident_Index', \\\n",
    "                index=['Accident_Severity'], \\\n",
    "                columns=['1st_Point_of_Impact'], \\\n",
    "                aggfunc=np.count_nonzero).transpose().plot(kind='bar',stacked=True)\n",
    "\n",
    "impact_lookup = pd.read_excel(\"data/metadata.xls\",sheet_name=\"1st Point of Impact\")\n",
    "xticks = impact_lookup['label'].values[0:len(impact_lookup['code'].values)-1]\n",
    "ax.set_xticklabels(xticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = pd.pivot_table(df_agg.drop(df_agg[df_agg['Age_Band_of_Driver'] == -1].index), values='Accident_Index', \\\n",
    "                index=['Accident_Severity'], \\\n",
    "                columns=['Age_Band_of_Driver'], \\\n",
    "                aggfunc=np.count_nonzero).transpose().plot(kind='bar',stacked=True)\n",
    "\n",
    "age_lookup = pd.read_excel(\"data/metadata.xls\",sheet_name=\"Age Band\")\n",
    "xticks = age_lookup['label'].values[0:len(age_lookup['code'].values)-1]\n",
    "ax.set_title('Distribution of Age band of Driver')\n",
    "ax.set_xlabel('Age band of Driver')\n",
    "ax.set_xticklabels(xticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge folium=0.5.0 --yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "df_agg['Latitude'] = df_agg['Latitude'].astype(float)\n",
    "df_agg['Longitude'] = df_agg['Longitude'].astype(float)\n",
    "heat_df = df_agg[df_agg['Accident_Severity'] == 1].loc[:,['Latitude', 'Longitude']]\n",
    "heat_df = heat_df.dropna(axis=0, subset=['Latitude','Longitude'])\n",
    "heat_data = heat_df.sample(len(heat_df)).values\n",
    "\n",
    "#Heatmap\n",
    "m = folium.Map(location=[54.251186, -4.463196],width=800,height=800, min_zoom=5, max_zoom=18, zoom_start=6, min_lat=48, max_lat=60, min_lon=-13, max_lon=4)\n",
    "HeatMap(heat_data.tolist(),radius=9.5).add_to(m)\n",
    "m\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "df_agg['Latitude'] = df_agg['Latitude'].astype(float)\n",
    "df_agg['Longitude'] = df_agg['Longitude'].astype(float)\n",
    "heat_df = df_agg[df_agg['Accident_Severity'] == 2].loc[:,['Latitude', 'Longitude']]\n",
    "heat_df = heat_df.dropna(axis=0, subset=['Latitude','Longitude'])\n",
    "heat_data = heat_df.sample(len(heat_df)).values\n",
    "\n",
    "#Heatmap\n",
    "m = folium.Map(location=[54.251186, -4.463196],width=800,height=800, min_zoom=5, max_zoom=18, zoom_start=6, min_lat=48, max_lat=60, min_lon=-13, max_lon=4)\n",
    "HeatMap(heat_data.tolist(),radius=9.5).add_to(m)\n",
    "m\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "df_agg['Latitude'] = df_agg['Latitude'].astype(float)\n",
    "df_agg['Longitude'] = df_agg['Longitude'].astype(float)\n",
    "heat_df = df_agg[df_agg['Accident_Severity'] == 3].loc[:,['Latitude', 'Longitude']]\n",
    "heat_df = heat_df.dropna(axis=0, subset=['Latitude','Longitude'])\n",
    "heat_data = heat_df.sample(len(heat_df)).values\n",
    "\n",
    "#Heatmap\n",
    "m = folium.Map(location=[54.251186, -4.463196],width=800,height=800, min_zoom=5, max_zoom=18, zoom_start=6, min_lat=48, max_lat=60, min_lon=-13, max_lon=4)\n",
    "HeatMap(heat_data.tolist(),radius=9.5).add_to(m)\n",
    "m\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "Independent Variables:\n",
    "\n",
    "1. Day of week\n",
    "2. Time (hour)\n",
    "3. Light Condition\n",
    "4. Weather Conditions\n",
    "5. Road conditions\n",
    "6. Number of Vehicles\n",
    "7. Number of Casualties\n",
    "8. Sex of Driver\n",
    "9. Age band of Driver\n",
    "10. 1st point of impact\n",
    "11. Road Type\n",
    "12. Hit Object in Carriageway\n",
    "13. Hit Object off Carriageway\n",
    "14. Vehicle Leaving Carriageway\n",
    "15. Special Conditions at Site\n",
    "16. Skidding and Overturning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_agg.loc[:,['Accident_Severity','Day_of_Week','Time','Light_Conditions','Weather_Conditions','Road_Surface_Conditions','Number_of_Vehicles'\\\n",
    "                      ,'Number_of_Vehicles','Sex_of_Driver','Age_Band_of_Driver','1st_Point_of_Impact','Road_Type','Hit_Object_in_Carriageway','Hit_Object_off_Carriageway'\\\n",
    "                      ,'Vehicle_Leaving_Carriageway','Special_Conditions_at_Site','Skidding_and_Overturning']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.drop(df[df['Accident_Severity'] == 3].sample(frac=0.95,replace=False,random_state=1).index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Accident_Severity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(df[df['Accident_Severity'] == 2].sample(frac=0.8,replace=False,random_state=1).index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Accident_Severity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time'] = pd.to_datetime(df['Time']).dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Missing Data for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weather Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['Weather_Conditions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df['Weather_Conditions'] == -1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Weather_Conditions'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Raod Surface Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Road_Surface_Conditions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df['Road_Surface_Conditions'] == -1].index)\n",
    "df['Road_Surface_Conditions'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sex of Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex_of_Driver'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[(df['Sex_of_Driver'] == -1) | (df['Sex_of_Driver'] == 3)].index)\n",
    "df['Sex_of_Driver'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Age band of Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age_Band_of_Driver'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[(df['Age_Band_of_Driver'] == -1)].index)\n",
    "df['Age_Band_of_Driver'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1st Point of Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['1st_Point_of_Impact'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(df[(df['1st_Point_of_Impact'] == -1)].index)\n",
    "df['1st_Point_of_Impact'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hit Object in Carriageway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Hit_Object_in_Carriageway'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(df[(df['Hit_Object_in_Carriageway'] == -1)].index)\n",
    "df['Hit_Object_in_Carriageway'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hit Object off Carriageway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Hit_Object_off_Carriageway'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(df[(df['Hit_Object_off_Carriageway'] == -1)].index)\n",
    "df['Hit_Object_off_Carriageway'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vehicle Leaving Carriageway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['Vehicle_Leaving_Carriageway'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(df[(df['Vehicle_Leaving_Carriageway'] == -1)].index)\n",
    "df['Vehicle_Leaving_Carriageway'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Special Conditions at Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Special_Conditions_at_Site'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(df[(df['Special_Conditions_at_Site'] == -1)].index)\n",
    "df['Special_Conditions_at_Site'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Skidding and Overturning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Skidding_and_Overturning'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(df[(df['Skidding_and_Overturning'] == -1)].index)\n",
    "df['Skidding_and_Overturning'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate Featuers and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Accident_Severity'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "The models used are:\n",
    "\n",
    "1. K Nearest Neighbor(KNN)\n",
    "2. Decision Tree\n",
    "3. Support Vector Machine\n",
    "4. Logistic Regression\n",
    "\n",
    "The Evaluation Metric:\n",
    "1. Jaccard Score\n",
    "2. F1-score\n",
    "3. Logloss (For Logistic Regression only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 4)\n",
    "print('Train Set: ', X_train.shape, y_train.shape)\n",
    "print('Test Set: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {'Algorithm': [],'Jaccard': [],'F1-score': [],'LogLoss': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = 10\n",
    "mean_acc = np.zeros((Ks-1))\n",
    "std_acc = np.zeros((Ks-1))\n",
    "\n",
    "for n in range(1,Ks):\n",
    "    KNN_model = KNeighborsClassifier(n_neighbors=n).fit(X_train, y_train)\n",
    "    KNN_yhat = KNN_model.predict(X_test)\n",
    "    mean_acc[n-1] = metrics.accuracy_score(y_test,KNN_yhat)\n",
    "    std_acc[n-1] = np.std(KNN_yhat==y_test)/np.sqrt(KNN_yhat.shape[0])\n",
    "\n",
    "print(mean_acc)\n",
    "print(\"The best accuracy was with\", mean_acc.max(), \"with k =\", mean_acc.argmax()+1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1,Ks),mean_acc,'g')\n",
    "plt.fill_between(range(1,Ks), mean_acc - 1 * std_acc, mean_acc + 1 * std_acc, alpha = 0.30)\n",
    "plt.legend(('Accuracy ', '+/- 1xstd'))\n",
    "plt.ylabel('Accuracy ')\n",
    "plt.xlabel('Number of Nabors (K)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_model = KNeighborsClassifier(n_neighbors=9).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "Tree_model = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\n",
    "Tree_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_yhat = Tree_model.predict(X_test)\n",
    "\n",
    "print('Decision Tree')\n",
    "print('F1-score: ', f1_score(y_test, DT_yhat, average = 'weighted'))\n",
    "print('Jaccard: ', jaccard_similarity_score(y_test, DT_yhat))\n",
    "\n",
    "test_results['Algorithm'].append('Decision Tree')\n",
    "test_results['Jaccard'].append(jaccard_similarity_score(y_test, DT_yhat))\n",
    "test_results['F1-score'].append(f1_score(y_test, DT_yhat, average = 'weighted'))\n",
    "test_results['LogLoss'].append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTree[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge pydotplus -y\n",
    "# !conda install -c conda-forge python-graphviz -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO\n",
    "import pydotplus\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "filename = \"severityTree.png\"\n",
    "featureNames = df.columns[1:]\n",
    "targetNames = df[\"Accident_Severity\"].unique().tolist()\n",
    "out=tree.export_graphviz(Tree_model,\n",
    "                         feature_names=featureNames,\n",
    "                         out_file=dot_data,\n",
    "                         class_names= ['1','2','3'],\n",
    "                         filled=True,\n",
    "                         special_characters=True,\n",
    "                         rotate=False)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png(filename)\n",
    "img = mpimg.imread(filename)\n",
    "plt.figure(figsize=(100, 200))\n",
    "plt.imshow(img,interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import jaccard_similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\n",
    "LR_yhat = LR.predict(X_test)\n",
    "LR_yhat_prob = LR.predict_proba(X_test)\n",
    "\n",
    "print('Logistict Regression')\n",
    "print('F1-score: ', f1_score(y_test, LR_yhat, average = 'weighted'))\n",
    "print('Jaccard: ', jaccard_similarity_score(y_test, LR_yhat))\n",
    "\n",
    "test_results['Algorithm'].append('LogisticRegression')\n",
    "test_results['Jaccard'].append(jaccard_similarity_score(y_test, LR_yhat))\n",
    "test_results['F1-score'].append(f1_score(y_test, LR_yhat, average = 'weighted'))\n",
    "test_results['LogLoss'].append(log_loss(y_test, LR_yhat_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
